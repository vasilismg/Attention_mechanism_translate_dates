# Attention mechanism - translate dates
Implementation of LSTM encoder-decoder architecture with attention mechanism. I applied the architecture to translate dates from human readable format to machine readable format, achieving accuracy of 80% on the testing set.

## Project
[Attention mechanism - translate dates](https://github.com/vgkortsas/Attention_mechanism_translate_dates/blob/master/Attention_mechanism_translate_dates.ipynb): Translate dates from human readable format ("3 May 1979", "5 April 09") to machine readable format ("1979-05-03", "2009-04-05").

The project is based on the programming assignment of deeplearning.ai, course Sequence models, Neural machine translation with attention. It was implemented in Keras and I reimplemented it using TensorFlow 1.14.

## Reference
[deeplearning.ai/Sequence models](https://www.coursera.org/learn/nlp-sequence-models/home/week/3)




